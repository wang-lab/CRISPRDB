{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import collections\n",
    "import os\n",
    "from keras.preprocessing import text\n",
    "import argparse,sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import scipy as sp\n",
    "from sklearn.metrics import  mean_squared_error, r2_score\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import merge, Embedding, Bidirectional, TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import *\n",
    "import dotmap\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_util import *\n",
    "def my_feature(df_model,learn_options):\n",
    "    '''\n",
    "    Args:\n",
    "        df_model:DataFrame数据，必须包含21mer信息\n",
    "        learn_options:训练模型、生成特征所需要的参数\n",
    "    returns: inputs,返回的是numpy array特征\n",
    "    '''\n",
    "    feature_sets = {}\n",
    "    feature_sets = featurize_data(df_model, learn_options)\n",
    "    inputs, dim, dimsum, feature_names = concatenate_feature_sets(feature_sets)\n",
    "    return inputs,dim,dimsum,feature_names,feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_1(y):\n",
    "    y[y > 1] = 1\n",
    "    y[y < 0] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Thesis/data/wt_feat_array_data.pkl', 'rb') as handle:\n",
    "    X_selected,y,features = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Thesis/data/esp_seq_data_array.pkl', 'rb') as handle:\n",
    "    test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "with open('data/esp_seq_data_array.pkl', 'rb') as handle:\n",
    "    esp_data = pickle.load(handle)\n",
    "with open('data/hf_seq_data_array.pkl', 'rb') as handle:\n",
    "    hf_data = pickle.load(handle)\n",
    "with open('data/wt_seq_data_array.pkl', 'rb') as handle:\n",
    "    wt_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_biofeat, y = esp_data['X_1'],esp_data['X_biofeat'],esp_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_util import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold,ShuffleSplit,StratifiedKFold,GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "import GPy\n",
    "import GPyOpt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "class GetBest(Callback):\n",
    "    \"\"\"Get the best model at the end of training.\n",
    "\t# Arguments\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        mode: one of {auto, min, max}.\n",
    "            The decision\n",
    "            to overwrite the current stored weights is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "\t# Example\n",
    "\t\tcallbacks = [GetBest(monitor='val_acc', verbose=1, mode='max')]\n",
    "\t\tmode.fit(X, y, validation_data=(X_eval, Y_eval),\n",
    "                 callbacks=callbacks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,filepath=None, monitor='val_loss', save_best=False,verbose=0,\n",
    "                 mode='auto', period=1):\n",
    "        super(GetBest, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.period = period\n",
    "        self.save_best = save_best\n",
    "        self.filepath = filepath\n",
    "        self.best_epochs = 0\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('GetBest mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "                \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            current = logs.get(self.monitor)\n",
    "            if current is None:\n",
    "                warnings.warn('Can pick best model only with %s available, '\n",
    "                              'skipping.' % (self.monitor), RuntimeWarning)\n",
    "            else:\n",
    "                if self.monitor_op(current, self.best):\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                              ' storing weights.'\n",
    "                              % (epoch + 1, self.monitor, self.best,\n",
    "                                 current))\n",
    "                    self.best = current\n",
    "                    self.best_epochs = epoch + 1\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                    #self.model.save(filepath, overwrite=True)\n",
    "                else:\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s did not improve.' %\n",
    "                              (epoch + 1, self.monitor)) \n",
    "                    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.verbose > 0:\n",
    "            print('Using epoch %05d with %s: %0.5f.' % (self.best_epochs, self.monitor,\n",
    "                                                       self.best))\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        #self.model.save(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_kf(X,y,split=0.15):\n",
    "    train_test_data = []\n",
    "    kf = ShuffleSplit(n_splits=10, test_size=0.15, random_state=33)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        train_test_data.append((X_train,X_test,y_train,y_test))\n",
    "    return train_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/esp_feat_data.pkl', 'rb') as f:\n",
    "    data_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data_list[0], np.array(data_list[1])\n",
    "feat_indext = np.loadtxt('data/esp_feat_index.txt').astype('int')\n",
    "X_selected = X[:,feat_indext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_kf(X,np.array(y))\n",
    "training_r2_scores = []\n",
    "testing_r2_scores = []\n",
    "spearmanrs = []\n",
    "loss = []\n",
    "models = []\n",
    "for data in data_list:\n",
    "    X_train,X_test, y_train,y_test = data[0],data[1],data[2],data[3]\n",
    "    data_train = xgb.DMatrix( X_train, y_train )\n",
    "    data_test = xgb.DMatrix( X_test, y_test )\n",
    "    param = {'learning_rate': 0.12, 'subsample': 0.7222222222222222, 'colsample_bytree': 0.6666666666666666, \n",
    "             'max_depth': 5, 'lambda': 700, 'objective': 'reg:squarederror', \n",
    "             'nthread': 18, 'booster': 'gbtree', 'seed': 1771}\n",
    "    watch_list = [(data_test, 'eval'), (data_train, 'train')]\n",
    "    bst = xgb.train( param, data_train, num_boost_round=700 ,evals=watch_list,early_stopping_rounds=3 )\n",
    "\n",
    "    y_train_pred = bst.predict(data_train)\n",
    "    y_test_pred = bst.predict(data_test)\n",
    "    raining_score = r2_score( y_train, y_train_pred )\n",
    "    testing_score = r2_score( y_test, y_test_pred )\n",
    "    spearmanr = sp.stats.spearmanr(y_test, y_test_pred)[0]   \n",
    "    mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    training_r2_scores.append(raining_score)\n",
    "    testing_r2_scores.append(testing_score)\n",
    "    spearmanrs.append(spearmanr)\n",
    "    loss.append(mse)\n",
    "    models.append(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.7369090120914517, 0.0013484501623500602),\n",
       " 'testing_r2_scores': (0.6741602042722457, 0.008407645462466747),\n",
       " 'spearmanr': (0.8309723102248828, 0.004579773609764182),\n",
       " 'loss': (0.011503698611460831, 0.00033907608093433015)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8320818569005929,\n",
       " 0.8286785617873859,\n",
       " 0.840044090263017,\n",
       " 0.8260184428428017,\n",
       " 0.8354593897202929,\n",
       " 0.8290010171733062,\n",
       " 0.8358978872735134,\n",
       " 0.8279861515661779,\n",
       " 0.8248368973183632,\n",
       " 0.8297188074033771]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.7369090120914517, 0.0013484501623500602),\n",
       " 'testing_r2_scores': (0.6741602042722457, 0.008407645462466747),\n",
       " 'spearmanr': (0.8309723102248828, 0.004579773609764182),\n",
       " 'loss': (0.011503698611460831, 0.00033907608093433015)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8320818569005929,\n",
       " 0.8286785617873859,\n",
       " 0.840044090263017,\n",
       " 0.8260184428428017,\n",
       " 0.8354593897202929,\n",
       " 0.8290010171733062,\n",
       " 0.8358978872735134,\n",
       " 0.8279861515661779,\n",
       " 0.8248368973183632,\n",
       " 0.8297188074033771]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.7371195937572591, 0.0012023099783800063),\n",
       " 'testing_r2_scores': (0.6749394827318866, 0.008179213083283057),\n",
       " 'spearmanr': (0.8315335878103474, 0.004797317859471297),\n",
       " 'loss': (0.011475680780733247, 0.0003139780377941402)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8307690773859965,\n",
       " 0.8286083782226864,\n",
       " 0.840217307114493,\n",
       " 0.8252908178026062,\n",
       " 0.8370227754990576,\n",
       " 0.8281813156049919,\n",
       " 0.838068583140455,\n",
       " 0.8303111506617731,\n",
       " 0.8274771204158126,\n",
       " 0.8293893522556008]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_kf(X_selected,y)\n",
    "training_r2_scores = []\n",
    "testing_r2_scores = []\n",
    "spearmanrs = []\n",
    "loss = []\n",
    "for data in data_list:\n",
    "    X_train,X_test, y_train,y_test = data[0],data[1],data[2],data[3]\n",
    "    model = linear_model.Ridge(alpha=104.32718311764306, fit_intercept=True, \n",
    "                             normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto')\n",
    "    #model = linear_model.LinearRegression(n_jobs=20)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = change_to_1(model.predict(X_train))\n",
    "    y_test_pred = change_to_1(model.predict(X_test))\n",
    "    mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    training_r2_scores.append(r2_score( y_train, y_train_pred ))\n",
    "    testing_r2_scores.append(r2_score( y_test, y_test_pred ))\n",
    "    spearmanrs.append(sp.stats.spearmanr(y_test,y_test_pred)[0])\n",
    "    loss.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.5826840011493762, 0.0017798479183876385),\n",
       " 'testing_r2_scores': (0.5795174214005189, 0.010104929383200612),\n",
       " 'spearmanr': (0.7698081619705687, 0.006797405517946372),\n",
       " 'loss': (0.014844643106943823, 0.0003997905148501007)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7703405622032071\n",
      "0.7643935412214841\n",
      "0.7848826572358722\n",
      "0.7625796922429551\n",
      "0.7775308642228411\n",
      "0.7660045188102381\n",
      "0.7731737706147075\n",
      "0.7694589738140001\n",
      "0.761825413613909\n",
      "0.7678916257264724\n"
     ]
    }
   ],
   "source": [
    "for i in spearmanrs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.5835083424678227, 0.001777749934034861),\n",
       " 'testing_r2_scores': (0.5801290776107423, 0.010119372894777011),\n",
       " 'spearmanr': (0.7701467356681887, 0.006758787527908767),\n",
       " 'loss': (0.014823066127737922, 0.00040068168703661196)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7707725234945665,\n",
       " 0.7647773642550586,\n",
       " 0.7851534970130917,\n",
       " 0.7627835344362617,\n",
       " 0.7777387271831084,\n",
       " 0.7662933996396077,\n",
       " 0.7733326605667139,\n",
       " 0.7700689325466341,\n",
       " 0.7622709929529221,\n",
       " 0.7682757245939218]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(fc_drop=0.4, batch_size=60, epochs=35, fc_num_hidden_layers=5, fc_num_units=100, fc_activation='3', optimizer='5'):\n",
    "    fc_activation = fc_activation_dict[str(fc_activation)]\n",
    "    optimizer = optimizer_dict[str(optimizer)]\n",
    "    \n",
    "    feat_input = Input(name = 'feat_input', shape = (X_train.shape[1],))\n",
    "    x = feat_input\n",
    "    for l in range(fc_num_hidden_layers):\n",
    "        x = Dense(fc_num_units, activation=fc_activation)(x)\n",
    "        x = Dropout(fc_drop)(x)\n",
    "\n",
    "\n",
    "    output = Dense(1, activation='linear',name='output')(x)\n",
    "    model = Model(inputs=[feat_input], outputs=[output])\n",
    "    model.compile(loss='mse',optimizer=optimizer(lr=0.001))\n",
    "\n",
    "    np.random.seed(1337)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    get_best_model = GetBest('models/esp_mlp.hd5',monitor='val_loss', verbose=1, mode='min')\n",
    "    model.fit([X_train], \n",
    "                 y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=2,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=False,\n",
    "                 callbacks=[get_best_model, early_stopping])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_activation_dict = {'1':'relu','2':'tanh', '3':'sigmoid', '4':'hard_sigmoid', '0':'elu'}\n",
    "initializer_dict = {'1':'lecun_uniform','2':'normal', '3':'he_normal', '0':'he_uniform'}\n",
    "optimizer_dict = {'1':SGD,'2':RMSprop, '3':Adagrad, '4':Adadelta,'5':Adam,'6':Adamax,'0':Nadam}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'fc_drop':0.2,\n",
    "'batch_size':35,\n",
    "'epochs':40,\n",
    "'fc_num_hidden_layers':4,\n",
    "'fc_num_units':400,\n",
    "'fc_activation':'3',\n",
    "'optimizer':'6'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_kf(X_selected,y)\n",
    "training_r2_scores = []\n",
    "testing_r2_scores = []\n",
    "spearmanrs = []\n",
    "loss = []\n",
    "for data in data_list:\n",
    "    X_train,X_test, y_train,y_test = data[0],data[1],data[2],data[3]\n",
    "    model = mlp_model(**param)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    training_r2_scores.append(r2_score( y_train, y_train_pred ))\n",
    "    testing_r2_scores.append(r2_score( y_test, y_test_pred ))\n",
    "    spearmanrs.append(sp.stats.spearmanr(y_test,y_test_pred)[0])\n",
    "    loss.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.7632138624587748, 0.01659869723608099),\n",
       " 'testing_r2_scores': (0.7018382546946994, 0.00774254308643637),\n",
       " 'spearmanr': (0.8457228621789528, 0.0029497993972957327),\n",
       " 'loss': (0.01052690595414275, 0.0003238017010849953)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010127593280284373\n",
      "0.010238676220832146\n",
      "0.009957035049880806\n",
      "0.010920643572781979\n",
      "0.010474505169919058\n",
      "0.010830294123300303\n",
      "0.010650946325839293\n",
      "0.010799664865294866\n",
      "0.010878046446646887\n",
      "0.010391654486647793\n"
     ]
    }
   ],
   "source": [
    "for i in loss:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8487986374203252,\n",
       " 0.847841295231162,\n",
       " 0.8490629771573187,\n",
       " 0.8435228616038357,\n",
       " 0.8475177601906175,\n",
       " 0.8411522181604375,\n",
       " 0.8451220319063756,\n",
       " 0.8458054089697685,\n",
       " 0.8408907471613889,\n",
       " 0.845528688256899]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in spearmanrs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_kf(X,X_biofeat,y,split=0.15):\n",
    "    train_test_data = []\n",
    "    kf = ShuffleSplit(n_splits=10, test_size=0.15, random_state=33)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_train_biofeat, X_test, X_test_biofeat = X[train_index],X_biofeat[train_index], X[test_index],X_biofeat[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        train_test_data.append((X_train,X_train_biofeat,X_test,X_test_biofeat,y_train,y_test))\n",
    "    return train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(batch_size=90, epochs=50, initializer='0',em_dim=44,em_drop=0.2,cnn_config=None,\n",
    "                fc_num_hidden_layers=3,fc_num_units=320, fc_drop=0.4,fc_activation='elu',optimizer=Adam,\n",
    "                learning_rate=0.001, validation_split=0.1,shuffle=False):\n",
    "    fc_activation = fc_activation_dict[str(fc_activation)]\n",
    "    cnn_config = cnn_config_dict[str(cnn_config)]\n",
    "    optimizer = optimizer_dict[str(optimizer)]\n",
    "    sequence_input = Input(name = 'seq_input', shape = (22,))\n",
    "\n",
    "    embedding_layer = Embedding(7,em_dim,input_length=22)\n",
    "    embedded = embedding_layer(sequence_input)\n",
    "    embedded = SpatialDropout1D(em_drop)(embedded)\n",
    "    x = embedded\n",
    "    \n",
    "    \n",
    "    \n",
    "    #CNN\n",
    "    convs = []\n",
    "    kernel_sizes = [int(f) for f in cnn_config[0].split(':')]\n",
    "    feature_maps = [int(f) for f in cnn_config[1].split(':')]\n",
    "    #构建多层卷积神经网络\n",
    "    for i in range(len(feature_maps)):\n",
    "        conv_layer = Conv1D(filters=feature_maps[i],\n",
    "                            kernel_size=kernel_sizes[i],\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)\n",
    "        conv_out = conv_layer(x)\n",
    "        pool_out = MaxPooling1D(pool_size=conv_layer.output_shape[1])(conv_out)\n",
    "        flatten_out = Flatten()(pool_out)\n",
    "\n",
    "        convs.append(flatten_out)\n",
    "\n",
    "    x = keras.layers.concatenate(convs) if len(convs) > 1 else convs[0]\n",
    "\n",
    "#     #RNN\n",
    "#     lstm = LSTM(rnn_units, dropout=rnn_drop, \n",
    "#                 kernel_regularizer='l2',recurrent_regularizer='l2',\n",
    "#                 recurrent_dropout=rnn_rec_drop, return_sequences=True)\n",
    "#     x = Bidirectional(lstm)(x)\n",
    "#     x = Flatten()(x)\n",
    "\n",
    "#     #生物学特征\n",
    "#     biological_input = Input(name = 'bio_input', shape = (X_train_biofeat.shape[1],))\n",
    "#     x = keras.layers.concatenate([x, biological_input])\n",
    "\n",
    "\n",
    "    for l in range(fc_num_hidden_layers):\n",
    "        x = Dense(fc_num_units, activation=fc_activation)(x)\n",
    "        x = Dropout(fc_drop)(x)\n",
    "    #finish model\n",
    "    mix_output = Dense(1, activation='linear',name='mix_output')(x)\n",
    "\n",
    "    #model = Model(inputs=[sequence_input, biological_input], outputs=[mix_output])\n",
    "    model = Model(inputs=[sequence_input], outputs=[mix_output])\n",
    "    model.compile(loss='mse', optimizer=optimizer(lr=0.001))\n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    get_best_model = GetBest('Model/esp_cnn.hd5',monitor='val_loss', verbose=1, mode='min')\n",
    "    #model.fit([X_train,X_train_biofeat], \n",
    "    model.fit([X_train], \n",
    "                 y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=2,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=False,\n",
    "                 callbacks=[get_best_model, early_stopping])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "config0 = ['1:2:3:4:5:6:7:8:9:10:11:12:13:14:15','20:20:20:20:20:20:20:20:20:20:20:20:20:20']\n",
    "config1 = ['1:2:3:4:5:6:7:8:9:10:11:12:13:14:15','20:20:20:20:20:20:20:40:40:40:40:40:40:40']\n",
    "config2 = ['1:2:3:4:5:6:7:8:9:10:11:12:13:14:15','20:20:20:20:40:40:40:40:80:80:80:80:80:80']\n",
    "config3 = ['1:2:3:4:5:6:7:8:9:10:11:12:13:14:15','40:40:40:40:40:40:40:40:80:80:80:80:80:80']\n",
    "\n",
    "\n",
    "cnn_config_dict = {'1':config1,'2':config2, '3':config3,'0':config0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'em_drop': 0.2, 'fc_drop': 0.2, 'batch_size': 70, 'epochs': 30, 'em_dim': 36, 'cnn_config': 2, \n",
    "         'fc_num_hidden_layers': 3, 'fc_num_units': 400, 'fc_activation': 1, 'optimizer': 6}\n",
    "\n",
    "#model_rnn = lstm_model(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_biofeat, y = esp_data['X_1'],esp_data['X_biofeat'],esp_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_list = load_data_kf(X,X_biofeat,y)\n",
    "training_r2_scores = []\n",
    "testing_r2_scores = []\n",
    "spearmanrs = []\n",
    "loss = []\n",
    "for data in data_list:\n",
    "    X_train,X_train_biofeat,X_test,X_test_biofeat,y_train,y_test = data[0],data[1],data[2],data[3],data[4],data[5]\n",
    "    model = cnn_model(**param)\n",
    "    #y_train_pred = model.predict([X_train,X_train_biofeat])\n",
    "    #y_test_pred = model.predict([X_test,X_test_biofeat])\n",
    "    y_train_pred = model.predict([X_train])\n",
    "    y_test_pred = model.predict([X_test])\n",
    "    training_score = r2_score( y_train, y_train_pred )\n",
    "    testing_score = r2_score( y_test, y_test_pred )\n",
    "    mse = mean_squared_error( y_test, y_test_pred)\n",
    "    spearmanr = sp.stats.spearmanr(y_test, y_test_pred)[0] \n",
    "\n",
    "    training_r2_scores.append(training_score)\n",
    "    testing_r2_scores.append(testing_score)\n",
    "    spearmanrs.append(spearmanr)\n",
    "    loss.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.759248989106773, 0.032414447233595274),\n",
       " 'testing_r2_scores': (0.6804897327919355, 0.016078851768164662),\n",
       " 'spearmanr': (0.8313350706208231, 0.008840303247433087),\n",
       " 'loss': (0.011279017406023512, 0.0005652284412441146)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01115373919841762\n",
      "0.010994457931076224\n",
      "0.010521041421004455\n",
      "0.011298116857012113\n",
      "0.010966085181726634\n",
      "0.01274333305749055\n",
      "0.01092944711277399\n",
      "0.011394402989552918\n",
      "0.011638507550669802\n",
      "0.011151042760510805\n"
     ]
    }
   ],
   "source": [
    "for i in loss:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8310248784708546,\n",
       " 0.8331721285102813,\n",
       " 0.8407636497220001,\n",
       " 0.8324440201960122,\n",
       " 0.8377643762402487,\n",
       " 0.8320308109795865,\n",
       " 0.8383716995210792,\n",
       " 0.8282071518244732,\n",
       " 0.8332817282145238,\n",
       " 0.8306395585490115]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(batch_size=90, epochs=50, initializer='0',em_dim=44,em_drop=0.2,\n",
    "                rnn_units=60, rnn_drop=0.6, rnn_rec_drop=0.1, fc_num_hidden_layers=3,\n",
    "                fc_num_units=320, fc_drop=0.4,fc_activation='elu',optimizer=Adam,learning_rate=0.001,\n",
    "                validation_split=0.1,shuffle=False):\n",
    "    #X_train,X_train_biofeat,y_train = data[0],data[1],data[2]\n",
    "    fc_activation = fc_activation_dict[str(fc_activation)]\n",
    "    initializer = initializer_dict[str(initializer)]\n",
    "    optimizer = optimizer_dict[str(optimizer)]\n",
    "    sequence_input = Input(name = 'seq_input', shape = (22,))\n",
    "\n",
    "    embedding_layer = Embedding(7,em_dim,input_length=22)\n",
    "    embedded = embedding_layer(sequence_input)\n",
    "    embedded = SpatialDropout1D(em_drop)(embedded)\n",
    "    x = embedded\n",
    "\n",
    "    #RNN\n",
    "    lstm = LSTM(rnn_units, dropout=rnn_drop, \n",
    "                kernel_regularizer='l2',recurrent_regularizer='l2',\n",
    "                recurrent_dropout=rnn_rec_drop, return_sequences=True)\n",
    "    x = Bidirectional(lstm)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "#     #生物学特征\n",
    "#     biological_input = Input(name = 'bio_input', shape = (X_train_biofeat.shape[1],))\n",
    "#     x = keras.layers.concatenate([x, biological_input])\n",
    "\n",
    "\n",
    "    for l in range(fc_num_hidden_layers):\n",
    "        x = Dense(fc_num_units, activation=fc_activation)(x)\n",
    "        x = Dropout(fc_drop)(x)\n",
    "    #finish model\n",
    "    mix_output = Dense(1, activation='linear',name='mix_output')(x)\n",
    "    #model = Model(inputs=[sequence_input, biological_input], outputs=[mix_output])\n",
    "    model = Model(inputs=[sequence_input], outputs=[mix_output])\n",
    "    model.compile(loss='mse', optimizer=optimizer(lr=0.001))\n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    get_best_model = GetBest('Model/esp_rnn.hd5',monitor='val_loss', verbose=1, mode='min')\n",
    "    #model.fit([X_train,X_train_biofeat], \n",
    "    model.fit([X_train], \n",
    "                 y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=2,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=False,\n",
    "                 callbacks=[get_best_model, early_stopping])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'em_drop': 0.2, 'rnn_drop': 0.2, 'rnn_rec_drop': 0.4, 'fc_drop': 0.4, \n",
    "         'batch_size': 70, 'epochs': 50, 'em_dim': 44, 'rnn_units': 70, 'fc_num_hidden_layers': 2, \n",
    "         'fc_num_units': 300, 'fc_activation': 3, 'optimizer': 6}\n",
    "\n",
    "#r = lstm_model(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44841 samples, validate on 4983 samples\n",
      "Epoch 1/50\n",
      " - 18s - loss: 0.2440 - val_loss: 0.0341\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03407, storing weights.\n",
      "Epoch 2/50\n",
      " - 16s - loss: 0.0447 - val_loss: 0.0344\n",
      "\n",
      "Epoch 00002: val_loss did not improve.\n",
      "Epoch 3/50\n",
      " - 16s - loss: 0.0382 - val_loss: 0.0321\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03407 to 0.03206, storing weights.\n",
      "Epoch 4/50\n",
      " - 16s - loss: 0.0345 - val_loss: 0.0283\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03206 to 0.02826, storing weights.\n",
      "Epoch 5/50\n",
      " - 16s - loss: 0.0284 - val_loss: 0.0233\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02826 to 0.02331, storing weights.\n",
      "Epoch 6/50\n",
      " - 16s - loss: 0.0251 - val_loss: 0.0227\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02331 to 0.02275, storing weights.\n",
      "Epoch 7/50\n",
      " - 16s - loss: 0.0231 - val_loss: 0.0200\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02275 to 0.02001, storing weights.\n",
      "Epoch 8/50\n",
      " - 16s - loss: 0.0209 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02001 to 0.01843, storing weights.\n",
      "Epoch 9/50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_list = load_data_kf(X,X_biofeat,y)\n",
    "training_r2_scores = []\n",
    "testing_r2_scores = []\n",
    "spearmanrs = []\n",
    "loss = []\n",
    "for data in data_list:\n",
    "    X_train,X_train_biofeat,X_test,X_test_biofeat,y_train,y_test = data[0],data[1],data[2],data[3],data[4],data[5]\n",
    "    model = lstm_model(**param)\n",
    "    #y_train_pred = model.predict([X_train,X_train_biofeat])\n",
    "    #y_test_pred = model.predict([X_test,X_test_biofeat])\n",
    "    y_train_pred = model.predict([X_train])\n",
    "    y_test_pred = model.predict([X_test])\n",
    "    training_score = r2_score( y_train, y_train_pred )\n",
    "    testing_score = r2_score( y_test, y_test_pred )\n",
    "    mse = mean_squared_error( y_test, y_test_pred)\n",
    "    spearmanr = sp.stats.spearmanr(y_test, y_test_pred)[0] \n",
    "\n",
    "    training_r2_scores.append(training_score)\n",
    "    testing_r2_scores.append(testing_score)\n",
    "    spearmanrs.append(spearmanr)\n",
    "    loss.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.7829373950316538, 0.009870604260570705),\n",
       " 'testing_r2_scores': (0.7097636412825155, 0.009920452627772193),\n",
       " 'spearmanr': (0.8491014520769882, 0.004744518983541059),\n",
       " 'loss': (0.010245622300480706, 0.0003493527744093642)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8492094220956927\n",
      "0.8466984549954044\n",
      "0.8548235819742712\n",
      "0.8455017188061741\n",
      "0.8559545327630784\n",
      "0.8446366187809149\n",
      "0.8560893272149397\n",
      "0.8502925401813233\n",
      "0.8427983791390835\n",
      "0.8450099448190005\n"
     ]
    }
   ],
   "source": [
    "for i in spearmanrs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN + Biofeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(batch_size=90, epochs=50, initializer='0',em_dim=44,em_drop=0.2,\n",
    "                rnn_units=60, rnn_drop=0.6, rnn_rec_drop=0.1, fc_num_hidden_layers=3,\n",
    "                fc_num_units=320, fc_drop=0.4,fc_activation='elu',optimizer=Adam,learning_rate=0.001,\n",
    "                validation_split=0.1,shuffle=False):\n",
    "    fc_activation = fc_activation_dict[str(fc_activation)]\n",
    "    initializer = initializer_dict[str(initializer)]\n",
    "    optimizer = optimizer_dict[str(optimizer)]\n",
    "    sequence_input = Input(name = 'seq_input', shape = (22,))\n",
    "\n",
    "    embedding_layer = Embedding(7,em_dim,input_length=22)\n",
    "    embedded = embedding_layer(sequence_input)\n",
    "    embedded = SpatialDropout1D(em_drop)(embedded)\n",
    "    x = embedded\n",
    "\n",
    "    #RNN\n",
    "    lstm = LSTM(rnn_units, dropout=rnn_drop, \n",
    "                kernel_regularizer='l2',recurrent_regularizer='l2',\n",
    "                recurrent_dropout=rnn_rec_drop, return_sequences=True)\n",
    "    x = Bidirectional(lstm)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "#     #生物学特征\n",
    "    biological_input = Input(name = 'bio_input', shape = (X_train_biofeat.shape[1],))\n",
    "    x = keras.layers.concatenate([x, biological_input])\n",
    "\n",
    "\n",
    "    for l in range(fc_num_hidden_layers):\n",
    "        x = Dense(fc_num_units, activation=fc_activation)(x)\n",
    "        x = Dropout(fc_drop)(x)\n",
    "    #finish model\n",
    "    mix_output = Dense(1, activation='linear',name='mix_output')(x)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, biological_input], outputs=[mix_output])\n",
    "    #model = Model(inputs=[sequence_input], outputs=[mix_output])\n",
    "    model.compile(loss='mse', optimizer=optimizer(lr=0.001))\n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    get_best_model = GetBest('Model/hf_rnn.hd5',monitor='val_loss', verbose=1, mode='min')\n",
    "    model.fit([X_train,X_train_biofeat], \n",
    "    #model.fit([X_train], \n",
    "                 y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=2,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=False,\n",
    "                 callbacks=[get_best_model, early_stopping])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'em_drop': 0.2, 'rnn_drop': 0.5, \n",
    "     'rnn_rec_drop': 0.5, 'fc_drop': 0.4, \n",
    "     'batch_size': 80, 'epochs': 46, \n",
    "     'em_dim': 48, 'rnn_units': 80, \n",
    "     'fc_num_hidden_layers': 2, 'fc_num_units': 300, \n",
    "     'fc_activation': 3, 'optimizer': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_kf(X,X_biofeat,y)\n",
    "training_r2_scores = []\n",
    "testing_r2_scores = []\n",
    "spearmanrs = []\n",
    "loss = []\n",
    "for data in data_list:\n",
    "    X_train,X_train_biofeat,X_test,X_test_biofeat,y_train,y_test = data[0],data[1],data[2],data[3],data[4],data[5]\n",
    "    model = lstm_model(**param)\n",
    "    y_train_pred = model.predict([X_train,X_train_biofeat])\n",
    "    y_test_pred = model.predict([X_test,X_test_biofeat])\n",
    "#     y_train_pred = model.predict([X_train])\n",
    "#     y_test_pred = model.predict([X_test])\n",
    "    training_score = r2_score( y_train, y_train_pred )\n",
    "    testing_score = r2_score( y_test, y_test_pred )\n",
    "    mse = mean_squared_error( y_test, y_test_pred)\n",
    "    spearmanr = sp.stats.spearmanr(y_test, y_test_pred)[0] \n",
    "\n",
    "    training_r2_scores.append(training_score)\n",
    "    testing_r2_scores.append(testing_score)\n",
    "    spearmanrs.append(spearmanr)\n",
    "    loss.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_r2_scores': (0.7966211082899555, 0.007287918270726685),\n",
       " 'testing_r2_scores': (0.7305182005037902, 0.008202604337021425),\n",
       " 'spearmanr': (0.8624301371241108, 0.004027002438459456),\n",
       " 'loss': (0.009513115137466098, 0.0002937367060889057)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "r['training_r2_scores'] = np.mean(training_r2_scores), np.std(training_r2_scores)\n",
    "r['testing_r2_scores'] = np.mean(testing_r2_scores),np.std(testing_r2_scores)\n",
    "r['spearmanr'] = np.mean(spearmanrs), np.std(spearmanrs)\n",
    "r['loss'] = np.mean(loss),np.std(loss)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009402830687045416\n",
      "0.009471316484137888\n",
      "0.009014022720839334\n",
      "0.009994259769559253\n",
      "0.009237383604450075\n",
      "0.00948051783547902\n",
      "0.009347444273015558\n",
      "0.009590112324740925\n",
      "0.010016949831851762\n",
      "0.009576313843541741\n"
     ]
    }
   ],
   "source": [
    "for i in loss:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8623787246455864,\n",
       " 0.8611237117010575,\n",
       " 0.866830537667525,\n",
       " 0.8567591763880864,\n",
       " 0.8687672729678532,\n",
       " 0.8614907966442155,\n",
       " 0.86755667461855,\n",
       " 0.8632395207054615,\n",
       " 0.8574422015211627,\n",
       " 0.8609042823635265]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(crispr)",
   "language": "python",
   "name": "crispr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
